{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060efaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(5000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "from pydub import AudioSegment\n",
    "from TTS.api import TTS\n",
    "from TTS.utils.manage import ModelManager\n",
    "import os\n",
    "import torch\n",
    "from TTS.utils.radam import RAdam\n",
    "import numpy.core.multiarray\n",
    "import shutil\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "torch.serialization.add_safe_globals([RAdam, numpy.core.multiarray.scalar])\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../Src/\")\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff9a81d-2d31-45b9-a4f2-f12f6183c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataSet = pd.read_csv('../Data/train_data.csv')\n",
    "#dataSet[dataSet['speaker_id'] == 'MMDM0']\n",
    "# outputDF = pd.read_csv('../Data/ttsOutputs/tts_models_en_ljspeech_tacotron2-DDC_generatedSentences.csv')\n",
    "# print(outputDF.shape[0])\n",
    "# uniqueSpeakers = outputDF['speakerId'].unique()\n",
    "# print(len(uniqueSpeakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9c0a27-d4ad-4074-8feb-7bd8bb600f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(dataset):\n",
    "    return pd.read_csv(f'../Data/{dataset}_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41cf41d-3b0b-48cf-8bd3-34eb6b306a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpeakers(df):\n",
    "    speakerIds = df['speaker_id']\n",
    "    return list(set(speakerIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3d89ff-453c-487a-933e-1d5a936f53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesBySpeaker(df,speakerId):\n",
    "    return df[df['speaker_id']==speakerId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefcf4c0-622e-45e9-b934-8782e0a4be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateAudio(speakerId, speakerDF):\n",
    "    if speakerDF.empty:\n",
    "        print(f\"Empty DataFrame for speaker {speakerId}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    finalAudioFile = f'../Data/concatenatedInputs/{speakerId}.wav'\n",
    "    audioData = speakerDF[speakerDF['filename'].str.endswith('.wav', na=False)]\n",
    "\n",
    "    if audioData.empty:\n",
    "        print(f\"No .wav files for speaker {speakerId}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    audioFileList = audioData['path_from_data_dir']\n",
    "\n",
    "    if not exists(finalAudioFile):\n",
    "        concat_audio = AudioSegment.empty()\n",
    "    else:\n",
    "        concat_audio = AudioSegment.from_wav(finalAudioFile)\n",
    "    for audioFile in audioFileList:\n",
    "        try:\n",
    "            audio = AudioSegment.from_wav(f'../Data/data/{audioFile}') + AudioSegment.silent(duration=1000)\n",
    "            concat_audio += audio\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {audioFile}: {e}\")\n",
    "\n",
    "    if len(concat_audio) > 0:\n",
    "        concat_audio.export(finalAudioFile, format='wav')\n",
    "    else:\n",
    "        print(f\"No valid audio for speaker {speakerId}, nothing exported.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce46883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSentenceFromFile(sentenceFile):\n",
    "    try:\n",
    "        with open(f'../Data/data/{sentenceFile}', 'r') as file:\n",
    "            return \" \".join(file.read().strip().split(\" \")[2:])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read sentence file {sentenceFile}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31952130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAndNormalizeAudio(tts, sentence, inputAudioFile, outputAudioFile):\n",
    "    try:\n",
    "        print(f\"Generating audio for sentence: {sentence}\")\n",
    "        tts.tts_with_vc_to_file(\n",
    "            text=sentence,\n",
    "            file_path=outputAudioFile,\n",
    "            speaker_wav=inputAudioFile\n",
    "        )\n",
    "        if not exists(outputAudioFile):\n",
    "            print(f\"Warning: Output file {outputAudioFile} was not created.\")\n",
    "            return False\n",
    "\n",
    "        # Normalize the generated audio\n",
    "        original_audio = AudioSegment.from_wav(inputAudioFile)\n",
    "        generated_audio = AudioSegment.from_wav(outputAudioFile)\n",
    "\n",
    "        gain = original_audio.dBFS - generated_audio.dBFS\n",
    "        normalized_audio = generated_audio.apply_gain(gain)\n",
    "\n",
    "        # Export the normalized audio back to the same file\n",
    "        normalized_audio.export(outputAudioFile, format='wav')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate or normalize audio: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d0bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveGeneratedSentences(speakerSentences, model_dir_name, modelDirectory):\n",
    "    outputFile = f'../Data/ttsOutputs/{model_dir_name}_generatedSentences.csv'\n",
    "    if speakerSentences:\n",
    "        try:\n",
    "            pd.DataFrame(speakerSentences).to_csv(\n",
    "                outputFile, index=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save generated sentences: {e}\")\n",
    "            shutil.rmtree(modelDirectory, ignore_errors=True)\n",
    "    if not exists(outputFile):\n",
    "        print(f\"Warning: Output file {outputFile} was not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efee4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGeneratedSentencesFromJson(speakerSentencesPath):\n",
    "    if exists(speakerSentencesPath):\n",
    "        with open(speakerSentencesPath, 'r') as f:\n",
    "            speakerSentences = json.load(f)\n",
    "    else:\n",
    "        speakerSentences = []\n",
    "    return speakerSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9449a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTTS(model):\n",
    "    try:\n",
    "        # Scoped override of torch.load\n",
    "        original_torch_load = torch.load\n",
    "        torch.load = lambda *args, **kwargs: original_torch_load(*args, weights_only=False, **kwargs)\n",
    "        tts = TTS(model_name=model, progress_bar=False, gpu=False)\n",
    "        result = tts\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load TTS model {model}: {e}\")\n",
    "        result = None\n",
    "    finally:\n",
    "        torch.load = original_torch_load  # Restore the original torch.load\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b6ff992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_speaker(speaker, trainDF, tts, modelDirectory):\n",
    "    outputFilePath = f'{modelDirectory}/{speaker}.wav'\n",
    "    if exists(outputFilePath):\n",
    "        print(f\"Output file {outputFilePath} already exists. Skipping.\")\n",
    "        return None\n",
    "    speakerDF = getFilesBySpeaker(trainDF, speaker)\n",
    "    if speakerDF.empty:\n",
    "        print(f\"No data for speaker {speaker}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    textFiles = speakerDF[speakerDF['path_from_data_dir'].str.contains('.TXT', na=False)]\n",
    "    if textFiles.empty:\n",
    "        print(f\"No valid sentences for speaker {speaker}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    chosenSentenceFile = np.random.choice(textFiles['path_from_data_dir'])\n",
    "    chosenSentence = readSentenceFromFile(chosenSentenceFile)\n",
    "    if not chosenSentence:\n",
    "        print(f\"Chosen sentence for speaker {speaker} is empty. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    audioFile = f'../Data/concatenatedInputs/{speaker}.wav'\n",
    "    if not exists(audioFile):\n",
    "        print(f\"Audio file for {speaker} does not exist. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    if generateAndNormalizeAudio(tts, chosenSentence, audioFile, outputFilePath):\n",
    "        return {'speakerId': speaker, 'generatedSentence': chosenSentence}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d40c3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawEnglishModelNames():\n",
    "    manager = ModelManager()\n",
    "    return [model for model in manager.list_models() if \"/en/\" in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6506f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAudioInBatches(speakers, trainDF, batch_size=10):\n",
    "    if trainDF.empty or not speakers:\n",
    "        print(\"Empty dataset or no speakers provided. Exiting.\")\n",
    "        return\n",
    "\n",
    "    englishModels = getRawEnglishModelNames()\n",
    "\n",
    "    if not englishModels:\n",
    "        print(\"No English models found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for model in englishModels:\n",
    "        print(f\"Processing model: {model}\")\n",
    "        model_dir_name = model.replace(\"/\", \"_\")\n",
    "        outputCSVFile = f'../Data/ttsOutputs/{model_dir_name}_generatedSentences.csv'\n",
    "        missing_speakers = list(speakers)\n",
    "        if exists(outputCSVFile):\n",
    "            outputDF = pd.read_csv(outputCSVFile)\n",
    "            processed_speakers = set(outputDF['speakerId'])\n",
    "            missing_speakers = list(set(speakers) - processed_speakers)  # <-- fix here\n",
    "            if not missing_speakers:\n",
    "                print(f\"All speakers processed for model {model}. Skipping.\")\n",
    "                continue\n",
    "            print(f'missing {len(missing_speakers)} speakers for model {model}')\n",
    "        \n",
    "        modelDirectory = f'../Data/ttsOutputs/{model_dir_name}'\n",
    "        os.makedirs(modelDirectory, exist_ok=True)\n",
    "        speakerSentencesPath = f'../Data/ttsOutputs/{model_dir_name}_generatedSentences.json'\n",
    "        speakerSentences = loadGeneratedSentencesFromJson(speakerSentencesPath)\n",
    "\n",
    "        for i in range(0, len(missing_speakers), batch_size):\n",
    "            batch = missing_speakers[i:i + batch_size]\n",
    "            print(f\"Processing batch {i // batch_size + 1}: {batch}\")\n",
    "\n",
    "            tts = generateTTS(model)\n",
    "            if tts is None:\n",
    "                print(f\"Failed to generate TTS for model {model}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(process_speaker, speaker, trainDF, tts, modelDirectory)\n",
    "                    for speaker in batch\n",
    "                ]\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        results.append(result)\n",
    "            # Write results after batch\n",
    "            speakerSentences.extend(results)\n",
    "            with open(speakerSentencesPath, 'w') as f:\n",
    "                json.dump(speakerSentences, f)\n",
    "\n",
    "            saveGeneratedSentences(speakerSentences, model_dir_name, modelDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7e707-4a0d-4f85-8a9c-ae9694d4a2e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trainDF = readCsv('train')\n",
    "    speakers = getSpeakers(trainDF)\n",
    "    if not exists('../Data/concatenatedInputs'):\n",
    "        os.makedirs('../Data/concatenatedInputs')\n",
    "        for speaker in speakers:\n",
    "            speakerDF = getFilesBySpeaker(trainDF, speaker)\n",
    "            concatenateAudio(speaker, speakerDF)\n",
    "    if not exists('../Data/ttsOutputs/'):\n",
    "        os.makedirs('../Data/ttsOutputs/')\n",
    "    \n",
    "    # Process speakers in batches\n",
    "    generateAudioInBatches(speakers, trainDF, batch_size=10)\n",
    "    \n",
    "    print(\"done\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768240ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: tts_models/multilingual/multi-dataset/xtts_v2\n",
      " 2: tts_models/multilingual/multi-dataset/xtts_v1.1\n",
      " 3: tts_models/multilingual/multi-dataset/your_tts\n",
      " 4: tts_models/multilingual/multi-dataset/bark\n",
      " 5: tts_models/bg/cv/vits\n",
      " 6: tts_models/cs/cv/vits\n",
      " 7: tts_models/da/cv/vits\n",
      " 8: tts_models/et/cv/vits\n",
      " 9: tts_models/ga/cv/vits\n",
      " 10: tts_models/en/ek1/tacotron2 [already downloaded]\n",
      " 11: tts_models/en/ljspeech/tacotron2-DDC [already downloaded]\n",
      " 12: tts_models/en/ljspeech/tacotron2-DDC_ph [already downloaded]\n",
      " 13: tts_models/en/ljspeech/glow-tts [already downloaded]\n",
      " 14: tts_models/en/ljspeech/speedy-speech [already downloaded]\n",
      " 15: tts_models/en/ljspeech/tacotron2-DCA [already downloaded]\n",
      " 16: tts_models/en/ljspeech/vits [already downloaded]\n",
      " 17: tts_models/en/ljspeech/vits--neon [already downloaded]\n",
      " 18: tts_models/en/ljspeech/fast_pitch [already downloaded]\n",
      " 19: tts_models/en/ljspeech/overflow [already downloaded]\n",
      " 20: tts_models/en/ljspeech/neural_hmm [already downloaded]\n",
      " 21: tts_models/en/vctk/vits [already downloaded]\n",
      " 22: tts_models/en/vctk/fast_pitch [already downloaded]\n",
      " 23: tts_models/en/sam/tacotron-DDC [already downloaded]\n",
      " 24: tts_models/en/blizzard2013/capacitron-t2-c50 [already downloaded]\n",
      " 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2 [already downloaded]\n",
      " 26: tts_models/en/multi-dataset/tortoise-v2 [already downloaded]\n",
      " 27: tts_models/en/jenny/jenny [already downloaded]\n",
      " 28: tts_models/es/mai/tacotron2-DDC\n",
      " 29: tts_models/es/css10/vits\n",
      " 30: tts_models/fr/mai/tacotron2-DDC\n",
      " 31: tts_models/fr/css10/vits\n",
      " 32: tts_models/uk/mai/glow-tts\n",
      " 33: tts_models/uk/mai/vits\n",
      " 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
      " 35: tts_models/nl/mai/tacotron2-DDC\n",
      " 36: tts_models/nl/css10/vits\n",
      " 37: tts_models/de/thorsten/tacotron2-DCA\n",
      " 38: tts_models/de/thorsten/vits\n",
      " 39: tts_models/de/thorsten/tacotron2-DDC\n",
      " 40: tts_models/de/css10/vits-neon\n",
      " 41: tts_models/ja/kokoro/tacotron2-DDC\n",
      " 42: tts_models/tr/common-voice/glow-tts\n",
      " 43: tts_models/it/mai_female/glow-tts\n",
      " 44: tts_models/it/mai_female/vits\n",
      " 45: tts_models/it/mai_male/glow-tts\n",
      " 46: tts_models/it/mai_male/vits\n",
      " 47: tts_models/ewe/openbible/vits\n",
      " 48: tts_models/hau/openbible/vits\n",
      " 49: tts_models/lin/openbible/vits\n",
      " 50: tts_models/tw_akuapem/openbible/vits\n",
      " 51: tts_models/tw_asante/openbible/vits\n",
      " 52: tts_models/yor/openbible/vits\n",
      " 53: tts_models/hu/css10/vits\n",
      " 54: tts_models/el/cv/vits\n",
      " 55: tts_models/fi/css10/vits\n",
      " 56: tts_models/hr/cv/vits\n",
      " 57: tts_models/lt/cv/vits\n",
      " 58: tts_models/lv/cv/vits\n",
      " 59: tts_models/mt/cv/vits\n",
      " 60: tts_models/pl/mai_female/vits\n",
      " 61: tts_models/pt/cv/vits\n",
      " 62: tts_models/ro/cv/vits\n",
      " 63: tts_models/sk/cv/vits\n",
      " 64: tts_models/sl/cv/vits\n",
      " 65: tts_models/sv/cv/vits\n",
      " 66: tts_models/ca/custom/vits\n",
      " 67: tts_models/fa/custom/glow-tts\n",
      " 68: tts_models/bn/custom/vits-male\n",
      " 69: tts_models/bn/custom/vits-female\n",
      " 70: tts_models/be/common-voice/glow-tts\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: vocoder_models/universal/libri-tts/wavegrad\n",
      " 2: vocoder_models/universal/libri-tts/fullband-melgan\n",
      " 3: vocoder_models/en/ek1/wavegrad [already downloaded]\n",
      " 4: vocoder_models/en/ljspeech/multiband-melgan [already downloaded]\n",
      " 5: vocoder_models/en/ljspeech/hifigan_v2 [already downloaded]\n",
      " 6: vocoder_models/en/ljspeech/univnet [already downloaded]\n",
      " 7: vocoder_models/en/blizzard2013/hifigan_v2 [already downloaded]\n",
      " 8: vocoder_models/en/vctk/hifigan_v2 [already downloaded]\n",
      " 9: vocoder_models/en/sam/hifigan_v2 [already downloaded]\n",
      " 10: vocoder_models/nl/mai/parallel-wavegan\n",
      " 11: vocoder_models/de/thorsten/wavegrad\n",
      " 12: vocoder_models/de/thorsten/fullband-melgan\n",
      " 13: vocoder_models/de/thorsten/hifigan_v1\n",
      " 14: vocoder_models/ja/kokoro/hifigan_v1\n",
      " 15: vocoder_models/uk/mai/multiband-melgan\n",
      " 16: vocoder_models/tr/common-voice/hifigan\n",
      " 17: vocoder_models/be/common-voice/hifigan\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: voice_conversion_models/multilingual/vctk/freevc24 [already downloaded]\n",
      "Processing model: tts_models/en/ek1/tacotron2\n",
      "missing 1 speakers for model tts_models/en/ek1/tacotron2\n",
      "Processing batch 1: [nan]\n",
      " > tts_models/en/ek1/tacotron2 is already downloaded.\n",
      " > vocoder_models/en/ek1/wavegrad is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-10\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:0\n",
      " | > fft_size:1024\n",
      " | > power:1.8\n",
      " | > preemphasis:0.99\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 2\n",
      " > Vocoder Model: wavegrad\n",
      "No data for speaker nan. Skipping.\n",
      "Processing model: tts_models/en/ljspeech/tacotron2-DDC\n",
      "missing 183 speakers for model tts_models/en/ljspeech/tacotron2-DDC\n",
      "Processing batch 1: ['FPJF0', 'MSMS0', 'MTXS0', 'FCEG0', 'MSRG0', 'MREE0', 'FSPM0', 'MCXM0', 'FJRB0', 'MDCM0']\n",
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      "Generating audio for sentence: Publicity and notoriety go hand in hand.\n",
      "Generating audio for sentence: How oily do you like your salad dressing?\n",
      "Generating audio for sentence: She had your dark suit in greasy wash water all year.\n",
      "Generating audio for sentence: According to my interpretation of the problem, two lines must be perpendicular.\n",
      "Generating audio for sentence: On unoccupied roadway the bottle shattered into a small amber flash.\n",
      "Generating audio for sentence: The fifth jar contains big, juicy peaches.\n",
      "Generating audio for sentence: She had your dark suit in greasy wash water all year.\n",
      "Generating audio for sentence: Shell shock caused by shrapnel is sometimes cured through group therapy.\n",
      " > Text splitted to sentences.\n",
      "['Publicity and notoriety go hand in hand.']\n",
      "Generating audio for sentence: Conservatism and traditionalism seem implied by what has just been said.\n",
      " > Text splitted to sentences.\n",
      "['She had your dark suit in greasy wash water all year.']\n",
      "Generating audio for sentence: The cartoon features a muskrat and a tadpole.\n",
      " > Text splitted to sentences.\n",
      "['According to my interpretation of the problem, two lines must be perpendicular.']\n",
      " > Text splitted to sentences.\n",
      "['On unoccupied roadway the bottle shattered into a small amber flash.']\n",
      " > Text splitted to sentences.\n",
      "['Conservatism and traditionalism seem implied by what has just been said.']\n",
      " > Text splitted to sentences.\n",
      "['The fifth jar contains big, juicy peaches.']\n",
      " > Text splitted to sentences.\n",
      "['How oily do you like your salad dressing?']\n",
      " > Text splitted to sentences.\n",
      "['She had your dark suit in greasy wash water all year.']\n",
      " > Text splitted to sentences.\n",
      "['The cartoon features a muskrat and a tadpole.']\n",
      " > Text splitted to sentences.\n",
      "['Shell shock caused by shrapnel is sometimes cured through group therapy.']\n",
      " > Processing time: 43.04534912109375\n",
      " > Real-time factor: 1.169203377367746\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Processing time: 44.009273052215576\n",
      " > Real-time factor: 1.1681261806414474\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Processing time: 44.27149295806885\n",
      " > Real-time factor: 1.1798126434907785\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Processing time: 44.42307901382446\n",
      " > Real-time factor: 1.1577041993518815\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Processing time: 44.66536593437195\n",
      " > Real-time factor: 1.193632946698721\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      "   > Decoder stopped with `max_decoder_steps` 10000\n",
      "   > Decoder stopped with `max_decoder_steps` 10000\n",
      "   > Decoder stopped with `max_decoder_steps` 10000\n",
      "   > Decoder stopped with `max_decoder_steps` 10000\n",
      "   > Decoder stopped with `max_decoder_steps` 10000\n",
      " > Processing time: 103.24149417877197\n",
      " > Real-time factor: 0.8849064537433227\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Processing time: 103.69585824012756\n",
      " > Real-time factor: 0.8888009120078104\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Processing time: 104.01549315452576\n",
      " > Real-time factor: 0.8915405759466418\n",
      " > Processing time: 104.07569694519043\n",
      " > Real-time factor: 0.8920565964025908\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Using model: freevc\n",
      " > Processing time: 104.60408806800842\n",
      " > Real-time factor: 0.8965855575378556\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      " > Loading pretrained speaker encoder model ...\n",
      "Loaded the voice encoder model on cpu in 135.04 seconds.\n",
      "Loaded the voice encoder model on cpu in 176.98 seconds.\n",
      "Loaded the voice encoder model on cpu in 177.77 seconds.\n",
      "Loaded the voice encoder model on cpu in 177.73 seconds.\n",
      "Loaded the voice encoder model on cpu in 121.88 seconds.\n",
      "Loaded the voice encoder model on cpu in 124.25 seconds.\n",
      "Loaded the voice encoder model on cpu in 126.07 seconds.\n",
      "Loaded the voice encoder model on cpu in 126.83 seconds.\n",
      "Loaded the voice encoder model on cpu in 188.35 seconds.\n",
      "Loaded the voice encoder model on cpu in 133.55 seconds.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
